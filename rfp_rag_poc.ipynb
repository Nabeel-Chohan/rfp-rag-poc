{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFP RAG PoC â€” AI Coding Agent\n",
    "\n",
    "This notebook contains a self-contained Proof-of-Concept (PoC) for a human-centric Retrieval-Augmented Generation (RAG) system for RFP handling. The PoC runs in Google Colab, uses **OpenAI 3.5 Turbo** for model calls, persists Knowledge Modules to a **JSON file** (`module_store.json`), accepts up to **two** uploaded past successful RFPs for ingestion, and exposes a minimal FastAPI backend + simple HTML UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fastapi uvicorn python-multipart openai pdfplumber nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Logic and Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import datetime\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "import openai\n",
    "\n",
    "# -- Knowledge Module Schema --\n",
    "MODULE_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"module_id\": {\"type\": \"string\"},\n",
    "        \"title\": {\"type\": \"string\"},\n",
    "        \"content\": {\"type\": \"string\"},\n",
    "        \"source_document_id\": {\"type\": \"string\"},\n",
    "        \"source_excerpt\": {\"type\": \"string\"},\n",
    "        \"confidence_level\": {\"type\": \"number\", \"minimum\": 0.0, \"maximum\": 1.0},\n",
    "        \"created_at\": {\"type\": \"string\", \"format\": \"date-time\"},\n",
    "        \"tags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "    },\n",
    "    \"required\": [\"module_id\", \"title\", \"content\", \"source_document_id\", \"created_at\"]\n",
    "}\n",
    "\n",
    "LOGS_FILE = 'logs.txt'\n",
    "\n",
    "def log_agent_call(agent_name: str, inputs: Dict[str, Any], outputs: Dict[str, Any], model_call: Dict = None):\n",
    "    log_entry = {\n",
    "        'agent': agent_name,\n",
    "        'input_summary': {k: (v[:100] + '...' if isinstance(v, str) and len(v) > 100 else v) for k, v in inputs.items()},\n",
    "        'output_summary': {k: (v[:100] + '...' if isinstance(v, str) and len(v) > 100 else v) for k, v in outputs.items()},\n",
    "        'timestamp': datetime.datetime.utcnow().isoformat(),\n",
    "    }\n",
    "    if model_call:\n",
    "        log_entry['model_call'] = model_call\n",
    "    with open(LOGS_FILE, 'a') as f:\n",
    "        f.write(json.dumps(log_entry) + '\\n')\n",
    "\n",
    "def generate_module_id(content: str) -> str:\n",
    "    \"\"\"Generates a deterministic module_id.\"\"\"\n",
    "    sha_hash = hashlib.sha1(content.encode()).hexdigest()[:8]\n",
    "    timestamp = datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "    return f\"KM-{sha_hash}-{timestamp}\"\n",
    "\n",
    "def distill_document_agent(raw_document_text: str, source_document_id: str) -> List[dict]:\n",
    "    \"\"\"Extracts Knowledge Modules from a raw document.\"\"\"\n",
    "    # Simple paragraph splitting\n",
    "    paragraphs = [p.strip() for p in raw_document_text.split('\\n\\n') if len(p.strip()) > 100]\n",
    "    modules = []\n",
    "    for p in paragraphs:\n",
    "        module_id = generate_module_id(p)\n",
    "        module = {\n",
    "            \"module_id\": module_id,\n",
    "            \"title\": p.split('\\n')[0][:50], # Simple title generation\n",
    "            \"content\": p,\n",
    "            \"source_document_id\": source_document_id,\n",
    "            \"source_excerpt\": p[:300],\n",
    "            \"confidence_level\": 0.8, # Default confidence\n",
    "            \"created_at\": datetime.datetime.utcnow().isoformat(),\n",
    "            \"tags\": [] # Tags can be added later\n",
    "        }\n",
    "        modules.append(module)\n",
    "    \n",
    "    log_agent_call('distill_document_agent', {'raw_document_text': raw_document_text, 'source_document_id': source_document_id}, {'modules_created': len(modules)})\n",
    "    return modules\n",
    "\n",
    "def retrieve_modules_agent(question: str, module_store: List[dict]) -> dict:\n",
    "    \"\"\"Retrieves relevant modules based on a question.\"\"\"\n",
    "    selected_modules = []\n",
    "    rationales = {}\n",
    "    question_keywords = set(re.findall(r'\\w+', question.lower()))\n",
    "\n",
    "    for module in module_store:\n",
    "        content_keywords = set(re.findall(r'\\w+', module['content'].lower()))\n",
    "        if question_keywords.intersection(content_keywords):\n",
    "            selected_modules.append(module['module_id'])\n",
    "            rationales[module['module_id']] = 'Keyword overlap found.'\n",
    "    \n",
    "    status = \"found\" if selected_modules else \"none\"\n",
    "    message = \"\" if status == \"found\" else \"No relevant modules found\"\n",
    "    \n",
    "    result = {\n",
    "      \"selected_module_ids\": selected_modules,\n",
    "      \"rationales\": rationales,\n",
    "      \"status\": status,\n",
    "      \"message\": message\n",
    "    }\n",
    "    log_agent_call('retrieve_modules_agent', {'question': question, 'module_count': len(module_store)}, result)\n",
    "    return result\n",
    "\n",
    "def assemble_answer_agent(question: str, selected_modules: List[dict]) -> dict:\n",
    "    \"\"\"Assembles an answer from selected modules.\"\"\"\n",
    "    if not selected_modules:\n",
    "        return {\"status\": \"insufficient\", \"reason\": \"No modules provided.\"}\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"Source (module_id: {m['module_id']}):\\n{m['content']}\" for m in selected_modules])\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant for RFP response generation. Your task is to answer the user's question using ONLY the provided text sources. Do not add any information that is not present in the sources.\n",
    "    Each sentence in your answer must be traceable to one or more of the provided module_ids.\n",
    "    Respond with a JSON object containing two keys: 'draft_answer' and 'sentence_traces'.\n",
    "    'draft_answer' should be a string containing the answer.\n",
    "    'sentence_traces' should be a list of objects, where each object has 'sentence' and 'module_ids' keys.\n",
    "\n",
    "    User Question: {question}\n",
    "\n",
    "    Sources:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You must generate a JSON object with 'draft_answer' and 'sentence_traces' keys.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        model_output = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        # Basic validation of the model's output\n",
    "        if 'draft_answer' not in model_output or 'sentence_traces' not in model_output:\n",
    "             raise ValueError(\"Model output missing required keys\")\n",
    "\n",
    "        used_module_ids = list(set(mid for trace in model_output.get('sentence_traces', []) for mid in trace.get('module_ids', [])))\n",
    "\n",
    "        result = {\n",
    "          \"draft_answer\": model_output['draft_answer'],\n",
    "          \"sentence_traces\": model_output['sentence_traces'],\n",
    "          \"used_module_ids\": used_module_ids,\n",
    "          \"status\": \"ok\",\n",
    "          \"reason\": \"\"\n",
    "        }\n",
    "        log_agent_call('assemble_answer_agent', {'question': question, 'module_count': len(selected_modules)}, result, {'prompt': prompt, 'response': model_output})\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        result = {\"status\": \"insufficient\", \"reason\": f\"Failed to generate answer: {e}\"}\n",
    "        log_agent_call('assemble_answer_agent', {'question': question, 'module_count': len(selected_modules)}, result)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FastAPI Application and UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from pydantic import BaseModel\n",
    "import pdfplumber\n",
    "import io\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "MODULE_STORE_FILE = 'module_store.json'\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "def read_module_store() -> List[dict]:\n",
    "    if not os.path.exists(MODULE_STORE_FILE):\n",
    "        return []\n",
    "    with open(MODULE_STORE_FILE, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_module_store(data: List[dict]):\n",
    "    with open(MODULE_STORE_FILE, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "html_ui = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>RFP RAG PoC</title>\n",
    "    <style>\n",
    "        body { font-family: sans-serif; margin: 2em; }\n",
    "        .container { max-width: 800px; margin: auto; }\n",
    "        .panel { border: 1px solid #ccc; padding: 1em; margin-bottom: 1em; border-radius: 5px; }\n",
    "        h2 { border-bottom: 1px solid #ccc; padding-bottom: 0.5em; }\n",
    "        pre { background-color: #f4f4f4; padding: 1em; border-radius: 3px; white-space: pre-wrap; word-wrap: break-word; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>RFP RAG PoC</h1>\n",
    "\n",
    "        <div class=\"panel\">\n",
    "            <h2>1. Ingest Documents</h2>\n",
    "            <form id=\"upload-form\" enctype=\"multipart/form-data\">\n",
    "                <input type=\"file\" name=\"files\" multiple required>\n",
    "                <button type=\"submit\">Upload and Ingest</button>\n",
    "            </form>\n",
    "            <h3>Ingestion Result:</h3>\n",
    "            <pre id=\"ingest-result\"></pre>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"panel\">\n",
    "            <h2>2. Query</h2>\n",
    "            <form id=\"query-form\">\n",
    "                <input type=\"text\" name=\"question\" style=\"width: 80%;\" required>\n",
    "                <button type=\"submit\">Ask</button>\n",
    "            </form>\n",
    "            <h3>Query Result:</h3>\n",
    "            <pre id=\"query-result\"></pre>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"panel\">\n",
    "            <h2>3. Admin</h2>\n",
    "            <button id=\"purge-button\">Purge Data</button>\n",
    "            <h3>Purge Result:</h3>\n",
    "            <pre id=\"purge-result\"></pre>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        document.getElementById('upload-form').addEventListener('submit', async (e) => {\n",
    "            e.preventDefault();\n",
    "            const formData = new FormData(e.target);\n",
    "            const response = await fetch('/ingest/upload', { method: 'POST', body: formData });\n",
    "            const result = await response.json();\n",
    "            document.getElementById('ingest-result').textContent = JSON.stringify(result, null, 2);\n",
    "        });\n",
    "\n",
    "        document.getElementById('query-form').addEventListener('submit', async (e) => {\n",
    "            e.preventDefault();\n",
    "            const question = e.target.elements.question.value;\n",
    "            const response = await fetch('/query', {\n",
    "                method: 'POST',\n",
    "                headers: { 'Content-Type': 'application/json' },\n",
    "                body: JSON.stringify({ question })\n",
    "            });\n",
    "            const result = await response.json();\n",
    "            document.getElementById('query-result').textContent = JSON.stringify(result, null, 2);\n",
    "        });\n",
    "\n",
    "        document.getElementById('purge-button').addEventListener('click', async () => {\n",
    "            if (confirm('Are you sure you want to delete all data?')) {\n",
    "                const response = await fetch('/purge', { \n",
    "                    method: 'POST',\n",
    "                    headers: { 'Content-Type': 'application/json' },\n",
    "                    body: JSON.stringify({ confirm: true })\n",
    "                });\n",
    "                const result = await response.json();\n",
    "                document.getElementById('purge-result').textContent = JSON.stringify(result, null, 2);\n",
    "            }\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def get_ui():\n",
    "    return html_ui\n",
    "\n",
    "@app.post(\"/ingest/upload\")\n",
    "async def ingest_upload(files: List[UploadFile] = File(...)):\n",
    "    if len(files) > 2:\n",
    "        raise HTTPException(status_code=400, detail=\"Cannot upload more than 2 files.\")\n",
    "    \n",
    "    module_store = read_module_store()\n",
    "    newly_created_modules = {}\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            contents = await file.read()\n",
    "            text = \"\"\n",
    "            if file.filename.endswith('.pdf'):\n",
    "                with pdfplumber.open(io.BytesIO(contents)) as pdf:\n",
    "                    text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
    "            else:\n",
    "                text = contents.decode('utf-8')\n",
    "            \n",
    "            modules = distill_document_agent(text, file.filename)\n",
    "            module_store.extend(modules)\n",
    "            newly_created_modules[file.filename] = [m['module_id'] for m in modules]\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=f\"Failed to process {file.filename}: {e}\")\n",
    "    \n",
    "    write_module_store(module_store)\n",
    "    return JSONResponse(content={\"created_modules\": newly_created_modules})\n",
    "\n",
    "@app.get(\"/modules\")\n",
    "async def get_modules():\n",
    "    return read_module_store()\n",
    "\n",
    "@app.post(\"/query\")\n",
    "async def query(request: QueryRequest):\n",
    "    module_store = read_module_store()\n",
    "    retrieval_result = retrieve_modules_agent(request.question, module_store)\n",
    "    \n",
    "    if retrieval_result['status'] == 'none':\n",
    "        return {\"retrieval\": retrieval_result, \"assembly\": None}\n",
    "    \n",
    "    selected_modules = [m for m in module_store if m['module_id'] in retrieval_result['selected_module_ids']]\n",
    "    assembly_result = assemble_answer_agent(request.question, selected_modules)\n",
    "    \n",
    "    return {\"retrieval\": retrieval_result, \"assembly\": assembly_result}\n",
    "\n",
    "@app.post(\"/purge\")\n",
    "async def purge(payload: Dict[str, bool]):\n",
    "    if not payload.get('confirm'):\n",
    "        raise HTTPException(status_code=400, detail=\"Confirmation not received.\")\n",
    "    \n",
    "    deleted_files = []\n",
    "    if os.path.exists(MODULE_STORE_FILE):\n",
    "        os.remove(MODULE_STORE_FILE)\n",
    "        deleted_files.append(MODULE_STORE_FILE)\n",
    "    if os.path.exists(LOGS_FILE):\n",
    "        os.remove(LOGS_FILE)\n",
    "        deleted_files.append(LOGS_FILE)\n",
    "        \n",
    "    return {\"status\": \"purged\", \"deleted_files\": deleted_files}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launch and Expose the Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import threading\n",
    "from google.colab import output\n",
    "import nest_asyncio\n",
    "\n",
    "def run_app():\n",
    "  # nest_asyncio is needed because Colab runs an event loop and uvicorn needs to run its own.\n",
    "  nest_asyncio.apply()\n",
    "  uvicorn.run(app, host='0.0.0.0', port=8000)\n",
    "\n",
    "# Run the app in a background thread to keep the notebook interactive\n",
    "thread = threading.Thread(target=run_app)\n",
    "thread.start()\n",
    "\n",
    "# Use colab's output to generate a public URL for the running app\n",
    "print(f\"Access the UI via the following public URL:\")\n",
    "print(output.eval_js('google.colab.kernel.proxyPort(8000)'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}